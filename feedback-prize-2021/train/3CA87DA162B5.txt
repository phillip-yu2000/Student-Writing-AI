Everyone says bigger is better. I don't believe that. When you go for something too big, many things could go wrong. A perfect example would be trying to develop a "smart" car. This would consist of making a car with a driverless driver, where the driver doesn't have to drive or pay attention at all. I don't think there should be driverless cars because it is too expensive, very dangerous, and if something does go wrong, who is too blame?

One reason we should not have driverless cars is because it is too expensive. It says in the text, "These smart-road systems worked suprisingly well, but they required massive upgrades to existing roads, something that was simply too expensive to be practical" (Paragraph 3). In order for us to have smart-cars, we need smart-roads. And just as I quoted up there, it is simply too expensive.

Aside from the financial part of driverless cars, the technology is not all where it needs to be. Making them very dangerous. It says in the text, "They can steer, accelerate, and brake themselves, but all are designed to notify the driver when the road ahead requires human skills, such as navigating through work zones and around accidents. This means the human driver must remain alert and be ready to take over when the situation requires" (Paragraph 7). This makes them dangerous because as soon as the driver becomes off-alert something bad could happen. Then, if they do not fix the problem by themselves, someone could get hurt. Which is why the driverless cars can be very unsafe, along with so many bad, unseen things happening.

Driverless cars make the roads unsafe. And because of this, if something does go wrong and someone gets hurt, who is too blame? It even asks this question in the text when it says, "If the technology fails and someone is injured, who is at fault-- the driver or the manufacturer?" (Paragraph 9). Who would be considered responsible and why? Adults who wrote this don't even know. This would cause a lot more fights and court cases. Or if they establish a law, would it effect the outcome on who would buy the car? To prevent all this from happening is another reason why we do not need smart-cars.

They says driverless cars will make driving safer. However, all I am seeing is reasons not to even try to make them. First, they are way too expensive. Second, they are too dangerous. And third, who is to blame if someone gets hurt? These are all reasons I believe we do not need to make or produce smart-cars.