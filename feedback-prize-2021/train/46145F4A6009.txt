The use of this technology called the Facial Action Coding System is not valuable to read students' emotional expressions. Not all expressions on a students face can be readable. Some students may have a natural expressions but show it differently. How accurate are these calculations for these expressions really?

In the article it states "Eckman has classified six basic emotions- happiness,surprise,anger,disgust,fear,and sadness" but what about emotions that go further and can be shown or thought to be as a basic expression ? More emotions are linked to the basic emotions if a person is to have guilt but show a face of anger but actually they are sad and cover it. The Facial Action Coding System just reads the expressions but not feelings of the students.

Some students have natural expressions. We have friends that have a natural 'mean mug' but are in reality happy or just calm this system could get it confused with an angry face. In the article it states "The facial expressions for each emotion are univeral.",meaning there is a wide range of emotions but could this system actually detect the difference between two similar faces of a person that makes the same face alot. In the article it also states

"For,example your frontalis pars lateralis muscle(above your eyes) raises your eyebrows when you're surprised; your orbicularis oris (around your mouth) tightens your lips to show anger." so for the students who have a natural facial structure it will most likely be harder to detect their actual emotion.

The article starts off by saying "Shes 83 percent happy,9 percent disgusted,6 percent fearful, and 2 percent angry." Although the system recognizes these expressions they are not actually reading her thoughts so they do not actually know the feelings of Mona Lisa and just are basing it off her expression. In this article it also states "Each expression is compared against a neutral face (showing no emotion)" but how accurate is this persons face against others they could be showing emotions also.

In conclusion, this is why the system should not be a value to read people expressions. It doesnt have a wide enough range for each person. The system bases it off others neutral faces. People structures are different from one another and have different meanings. The system does not give you actual feelings and just read faces.       